{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dense Layer* or *fully connected layer*\n",
    "All neurons in a layer are connected every neuron in the previous layer.\n",
    "\n",
    "### Input Layer\n",
    "The inputs of a perceptron are fed to special passthrough neurons called *input neurons*\n",
    "\n",
    "### Bias feature\n",
    "Generally x0 = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "One of the simplest ANN architectures, based on a slightly differente articial neuron callsed *threshold logic unit (TLU)* or *linear threshold unir (LTU)*. A single TLU can be used for a simple linear binary classification. It computes a linear combination of the inputs, and if the result exceeds a threshold, it output a positive class.\n",
    "\n",
    "A Perceptron is simply composed of a single layer of TLUs, with each TLE connected to all the inputs.\n",
    "\n",
    "$$ h_{W, b} = \\phi(XW + b) $$\n",
    "\n",
    "- X matrix of input features.\n",
    "- W weight matrix, contains all the connection weights. It has one row per input input neuron and one column per artifical neuron in the layer.\n",
    "- b bias vector, it has one bias term per artifical neuron.\n",
    "- $\\phi$ activation function\n",
    "\n",
    "### How the perceptron is trained?\n",
    "\n",
    "Perceptrons are trained using a variant of this rule that takes into account the error made by the network when it makes prediction\n",
    "\n",
    "$$\n",
    "w_{i,j}^{(nest step)} = w_{i,j} + \\alpha (y_{j} - \\hat y_{j})x_{i}\n",
    "$$\n",
    "\n",
    "- w_{i,j} is the connection weight between the $i^{th}$ input neuron and the $j^{th}$ output neuron\n",
    "- $x_{i} i^{th}$ vale of the current training instance\n",
    "- $\\hat y_{j} j^{th}$ output neuron for the current training instance\n",
    "- $\\hat y_{j}$ target output\n",
    "- $\\alpha$ learning rate\n",
    "\n",
    "The perceptron learning algorithm strongly resembles Stochastic Gradient Descent. Note that contrary to Logistic Regression classifiers, Perceptron do not output a class probability; rather, they make predictions based on a hard threshold. This is one reason to prefer LR over Perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2,3)]\n",
    "# filtrando todas as iris classificadas como 0, gerando um array de boolean e convertendo pra int\n",
    "y = (iris.target == 0).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0])"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "per_clf = Perceptron()\n",
    "per_clf.fit(X,y)\n",
    "\n",
    "y_pred = per_clf.predict([[2,0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some  of the limitations of Perceptrons can be eliminated by stacking multiple Perceptrons. The resulting ANN is called a *Multilayer Perceptron (MLP)*. An MPL can solve the XOR problem. An MPL is composed of one *input layer*, one or more layers of TLUs, called hidden layers, and one final layer of TLUs called the *output layer*.\n",
    "\n",
    "When an ANN contains a deep stack of hidden layers, it is called a *deep neural network* (DNN). For many years researchers struggled to find a way to train MLPs, without success. But in 1986, David Rumelhad, Geoffrey Hinton and Ronald Willians publishd a **groundbreaking paper** that introduce the *backpropagation* training algorithm, which is still used today. In short, it for computing the gradients automatically: in just two passes through the network (one forward, one backward), the backpropagation is able to compute the gradient of the network's error with regard to every single model parameter. In other words, it can find out how each\n",
    "connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbaseconda1045c210760f452dabbfd0c15f151bd0",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}